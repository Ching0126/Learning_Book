# 学习内容汇报

## 1. 了解智能云盒的物理传感器部分的工作流程

![image-20251029132906483](C:\Users\qiyi\AppData\Roaming\Typora\typora-user-images\image-20251029132906483.png)

1. ***main.cpp* 中的main() 函数初始化 QApplication 并创建 MainWindow 实例**

   MainWindow 构造函数调用首先进行界面的全局配置

   initEdgeLayout()实现初始化页面布局以及日志信息悬停窗口，初始化成功后，左下角显示初始化成功。

   initContent() 初始化目录

   连接主页的信号与主窗口的槽函数，实现页面导航

2.  **T_carbonDioxide.cpp首页的实现**

   初始化物理传感器的页面布局

   调用mqttsubscriber从物理传感器接受数据，接受物理传感器数据

   连接自身槽函数

3. **C_mqttsubscriber.cpp获取物理传感器数据**

   构造函数中实现了mqtt的连接

   onMessageReceived（）核心数据处理逻辑，根据不同的主题处理数据

## 2. 复现了一个简单的使用yolo8的图像识别项目

![image-20251029131504038](C:\Users\qiyi\AppData\Roaming\Typora\typora-user-images\image-20251029131504038.png)

### **总体架构**

- 后端使用 `Flask` 提供网页与视频流接口。
- 模型采用 `Ultralytics YOLOv8` 加载权重 `yolov8n.pt` 推理。
- 视频采集与图像处理使用 `OpenCV`，以 MJPEG 流推送到浏览器。

### **关键组件**

- `Camera` 类：初始化摄像头、读取帧、释放资源。
- `FaceDetector` 类：加载模型（`YOLO(self.model_path)`）、对帧进行检测并绘制标注。
- `generate_frames()`：编码 JPEG 并以 `multipart/x-mixed-replace` 流式输出。
- 路由：
  - `/` 渲染首页模板。
  - `/video_feed` 输出实时视频流。

### **运行流程**

- 访问 `/` 初始化摄像头与模型。
- 循环读取帧 → 调用 YOLOv8 推理 → 绘制检测结果。
- 将编码后的帧通过 MJPEG 流回传，前端 `<img>` 实时显示。

## 3. 《面向智慧教室的教学手势智能识别研究》

### 一、现有技术的核心痛点

1. **静态手势识别困境**：传统依赖手部关键点的识别方法，因教室视频中教师手部目标小、画质模糊，易出现关键点识别混乱，无法准确区分数字手势、OK、竖大拇指等静态手势；
2. **姿态与连续手势识别不足**：基于人体关键点的姿态估计方案，仅能识别单一手势，无法处理点头、摇头、鼓掌等连续性手势；且传统热图编码 / 解码存在量化误差，影响关键点检测精度。

### 二、关键技术解决方案

论文围绕 “静态手势识别 - 人体关键点检测 - 连续性手势识别” 三大核心模块，设计针对性改进方案，形成完整技术链：

#### （一）电子白板检测：改进 Faster R-CNN

##### Faster R-CNN

1. **特征提取**

- 支持任意尺寸输入图像，预处理阶段统一缩放至固定尺寸（如 M×N），再输入特征提取网络（以 VGG16 为例）。

- VGG16 含 13 个卷积层（kernel size=3、padding=1、stride=1）和 4 个池化层（kernel size=2、stride=2），经**卷积操作后图像尺寸不变**，每轮**池化后尺寸减半**，最终输出通道数 512、尺寸 M/16×N/16 的特征图。

  ![image-20251029170303301](C:\Users\qiyi\AppData\Roaming\Typora\typora-user-images\image-20251029170303301.png)

2. **RPN 区域生成网络**

- **核心创新**：替代传统 Selective Search 生成候选框，提升速度；引入 **Anchor 机制**，。以60*40的feature  map为例，在特征图每像素生成 9 个不同大小和不同比例的 Anchor，通过映射关系对应原图区域。

- **流程**：特征图先经 3×3 卷积调整通道数（256 维），再分两个 1×1 卷积分支：分类分支（18 维，**判断 Anchor 为前景 / 后景**）、回归分支（36 维，**预测 Anchor 与真实框的偏移量**），两个分支的输出结 合anchors 信息经过筛选后会得到一系列proposals。

- 筛选后，一张特征图采样 256 个 Anchor（正样本≤128），用二值交叉熵损失（分类）和 Smooth L1 损失（回归）优化。

  ![image-20251029171907013](C:\Users\qiyi\AppData\Roaming\Typora\typora-user-images\image-20251029171907013.png)

3.  **ROI Pooling 和检测头（Bbox Head）**
4.  ![image-20251029191944657](C:\Users\qiyi\AppData\Roaming\Typora\typora-user-images\image-20251029191944657.png)

- **ROI Pooling**：。ROI阶段的目的是让所有目标建议框都能共享一个feature map，并保 持输出大小一致。将 RPN 输出的候选框通过缩放因子映射到特征图，达到共享特征图；为了保持输出大小，将特征图划分为 7×7 网格并最大池化，选取最大像素点作为输出，统一输出尺寸，实现特征共享。

- **检测头**：经全连接层后分两个分支：分类分支（判定目标类别）、回归分支（优化候选框位置）；若 IoU＞0.5 为正样本，最后随机采样 512 个样本训练，正样本<0.5。总损失含分类损失（交叉熵）和回归损失（Smooth L1）。

##### 电子白板检测：识别指示性手势之指向电子白板

1. **数据集建设**：创建电子白板专用数据集**CCNU2020**，包含 389 张图像、513 个电子白板标注，覆盖不同角度、尺寸的场景；
2. **基础模型优化**：以目标检测算法 Faster R-CNN 为基线模型，从特征提取、特征融合、预训练权重三方面进行优化：
   - **特征提取网络改进**：将原 Faster R-CNN 的特征提取网络替换为 **ResNet50-D**。原 ResNet50 下采样时主分支用 1×1 卷积（stride=2）易造成 3/4 信息损失，ResNet50-D 则调整为：主分支先通过 1×1 卷积调整通道数，再用 3×3 卷积（stride=2）完成下采样；残差分支在 1×1 卷积前加入 2×2 平均池化（stride=2）实现下采样，有效减少信息丢失。
   - **特征融合增强**：引入 **FPN**（特征金字塔网络），在 RPN 阶段前融合 ResNet50-D 各阶段特征。深层特征（如 Conv5 输出的 P5）携带丰富语义信息，浅层特征（如 Conv2 输出的 P2）保留更多细节信息，通过上采样与横向连接将不同尺度特征融合，使模型同时具备检测不同距离、尺寸电子白板的能力，提升目标位置回归精度。
   - **预训练权重加载**：加载通过 SSLD（简单半监督知识蒸馏）得到的 ResNet50-D 预训练权重。该权重由教师模型在 500 万张图像（ImageNet22K 挖掘 400 万张 + ImageNet-1K 训练集）上指导学生模型训练，再经 ImageNet-1K 微调得到，可帮助模型更快收敛并提升性能。

3. **效果**：电子白板检测 mAP（均值平均精度）达**99.8%**，FPS（每秒处理帧数）提升至 12.51，满足实时检测需求。

#### （二）手部静态手势分类：优化 GoogLeNet 网络

1. **数据集建设**：创建教学场景专用静态手势数据集**CCNU-Hand**，涵盖 8 类常见手势（数字 1/3/4/5/6、Yeah、竖大拇指、OK），原始数据 1731 张，经Sample Pairing 这种叠加式的线上图像增强、Mixup数据增强。

1. **模型优化方向**：

- 基础框架：采用以 Inception 模块为核心的 GoogLeNet，借助多尺度卷积核强化特征提取能力；

- 关键改进：Inception模块搭建网络的过程中， 在进行大小为3*3或5*5的卷积操作时，都会首先使用1*1卷积核来降维；加入**空间注意力机制（SAM）**，引导模型聚焦手部关键特征，减少背景干扰；结合**Mixup 线上数据增强**，通过样本与标签的比例融合降低过拟合，提升模型鲁棒性；

1. **效果**：在 CCNU-Hand 验证集上准确率达**87.5%**，性能优于 VGG16、ResNet34 等经典模型。

#### （三）人体关键点检测：HR_SENet+DARK 组合方案

##### HRNet

为手势识别提供教师肢体坐标信息，核心特点是 “始终保持高分辨率”：

![image-20251029203504418](C:\Users\qiyi\AppData\Roaming\Typora\typora-user-images\image-20251029203504418.png)

- **网络结构**：含 4 个 Stage，分支数随 Stage 递增（1→2→3→4），同一分支分辨率一致；每个 Stage（除最后一个）结束时，由最小分辨率分支下采样生成新分支，且各 Stage 间进行跨分辨率特征融合（高分辨率下采样、低分辨率上采样），兼顾全局语义与细节信息。
- **任务适配**：针对人体关键点检测采用 HRNetV1，即输出最高分辨率融合特征图；通过 MSE 损失函数监督预测热图与真实热图（以关键点为中心的二维高斯分布）的偏差。

##### 人体关键点检测

1. **核心目标**：提高教师人体关键点检测精度，为指示性、连续性手势识别提供可靠坐标数据；

1. **模型优化方向**：

- 基础框架：以 HRNet 为基准（保持高分辨率特征，避免下采样导致的位置信息丢失）；

- 两大改进：① 嵌入**SE 通道注意力机制**，在残差模块中增加 SE 层，让模型关注不同通道的重要性，构建 HR_SENet 网络；② 引入**分布式感知坐标描述（DARK）**，通过高斯分布调整预测热图、亚像素泰勒展开优化坐标计算，解决传统热图的量化误差问题；

1. **效果**：在公开数据集 MPII 上，PCKh（人体关键点检测评价指标）达**91.0%**，头部、肩部、肘部等关键点识别精度优于 HRNet、CPM 等主流模型，且能在真实课堂中准确检测教师上半身关键点。

1. 

#### （四）综合手势识别：分类型设计流程

论文按 “时序性” 将教学手势分为两类，分别设计识别方案：

| 手势类型       | 包含手势                                 | 识别方案                                                     | 效果                                                    |
| -------------- | ---------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------- |
| 连续性教学手势 | 点头、摇头、鼓掌、双手交叉胸前、小臂摆动 | 1. 数据集：基于 NTU-RGB+D 构建**CCNU-ACT**数据集（4717 个样本，含 5 类连续手势）；2. 模型：采用 “多层感知机 + CNN” 搭建分类模型，输入连续 300 帧关键点数据；3. 节拍性手势（小臂摆动）：通过计算连续多帧肢体角度变化幅度判定。 | 分类模型测试集准确率**88.5%**，小臂摆动识别准确率 73.1% |
| 单帧教学手势   | 指向电子白板、手部静态手势               | 1. 指向电子白板：结合电子白板位置与肢体角度（大臂与身体夹角 > 30°、大臂与小臂夹角 > 90°）计算；2. 静态手势：通过 “手肘 - 手腕向量定位手部”，截取图像后输入优化后的 GoogLeNet_SAM+Mixup 模型。 | 指向电子白板准确率 86.7%，静态手势准确率 87.5%          |

1. **整体流程**：输入教学视频后，同步执行电子白板检测与人体关键点检测，根据检测结果触发单帧或连续性手势识别逻辑，最终输出指示性、描述性、节拍性三类手势识别结果。

### 三、总结

- 构建了智慧教室专用手势数据集（CCNU-Hand、CCNU2020、CCNU-ACT），填补了场景化数据的空白；

- 提出 “SAM+Mixup 优化 GoogLeNet”“HR_SENet+DARK” 等模型，解决了静态手势识别混乱、关键点量化误差等问题；

- 实现了从单帧到连续性手势的全场景识别，形成可落地的教学手势识别系统。



